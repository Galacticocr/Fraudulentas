# -*- coding: utf-8 -*-
"""Fraudulentas2USV.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1TGqCfNnAQ93PGmQBtm1b8IiveJIK61YV
"""

import pandas as pd
import numpy as np

# Definir el número de transacciones
num_transacciones = 1000

# Generar datos aleatorios para cada característica
monto = np.random.randint(100, 1000001, num_transacciones)
ubicacion = np.random.choice(['San José', 'Alajuela', 'Cartago', 'Heredia', 'Guanacaste', 'Puntarenas', 'Limón'], num_transacciones)
historial = np.random.randint(1, 101, num_transacciones)  # Historial simplificado (1-100)
tipo_cuenta = np.random.choice(['Corriente', 'Ahorro', 'Crédito'], num_transacciones)
metodo_pago = np.random.choice(['Tarjeta', 'Efectivo', 'Transferencia'], num_transacciones)

# Generar etiquetas de fraude (10% fraudulentas)
fraude = np.random.choice([0, 1], num_transacciones, p=[0.9, 0.1])

# Crear un diccionario con los datos
data = {
    'Monto': monto,
    'Ubicación': ubicacion,
    'Historial': historial,
    'Tipo Cuenta': tipo_cuenta,
    'Método Pago': metodo_pago,
    'Fraude': fraude
}

# Crear un DataFrame de pandas
df = pd.DataFrame(data)

# Guardar el DataFrame en un archivo CSV
df.to_csv('transacciones_bancarias.csv', index=False)

print("Archivo 'transacciones_bancarias.csv' creado con éxito.")

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, OneHotEncoder

# 1. Carga del dataset
df = pd.read_csv('transacciones_bancarias.csv')

# 2. Exploración de datos
print(df.head())  # Mostrar las primeras filas
print(df.info())  # Información sobre las columnas y tipos de datos
print(df.describe())  # Estadísticas descriptivas
print(df['Fraude'].value_counts())  # Balance de clases

# 3. Manejo de valores faltantes (asumiendo que no hay valores nulos en este ejemplo)
# Si hay valores nulos, puedes usar:
# df.dropna()  # Eliminar filas con valores nulos
# df.fillna(df.mean())  # Imputar con la media de la columna

# 4. Codificación de variables categóricas
encoder = OneHotEncoder(handle_unknown='ignore')
encoded_features = encoder.fit_transform(df[['Ubicación', 'Tipo Cuenta', 'Método Pago']]).toarray()
encoded_df = pd.DataFrame(encoded_features)
df = df.drop(['Ubicación', 'Tipo Cuenta', 'Método Pago'], axis=1)
df = pd.concat([df, encoded_df], axis=1)

# 5. Normalización/Estandarización
scaler = StandardScaler()
df[['Monto', 'Historial']] = scaler.fit_transform(df[['Monto', 'Historial']])

# 6. División del dataset
X = df.drop('Fraude', axis=1)
y = df['Fraude']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

print("Preprocesamiento de datos completado.")

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.svm import SVC
from sklearn.metrics import classification_report, confusion_matrix  # Import metrics

# 1. Carga del dataset
df = pd.read_csv('transacciones_bancarias.csv')

# 2. Codificación de variables categóricas (One-Hot Encoding)
encoder = OneHotEncoder(handle_unknown='ignore', sparse_output=False) # Recomendado: sparse_output=False
encoded_features = encoder.fit_transform(df[['Ubicación', 'Tipo Cuenta', 'Método Pago']])
encoded_df = pd.DataFrame(encoded_features, columns=encoder.get_feature_names_out(['Ubicación', 'Tipo Cuenta', 'Método Pago']))
df = df.drop(['Ubicación', 'Tipo Cuenta', 'Método Pago'], axis=1)
df = pd.concat([df, encoded_df], axis=1)

# 3. División del dataset
X = df.drop('Fraude', axis=1)
y = df['Fraude']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y) # Stratify for balanced classes

# 4. Escalado de datos
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# 5. Crear y entrenar el modelo SVM
model = SVC(kernel='rbf', random_state=42)
model.fit(X_train, y_train)

# 6. Evaluación del modelo
y_pred = model.predict(X_test)
print(classification_report(y_test, y_pred)) # More detailed report
print(confusion_matrix(y_test, y_pred))      # Confusion matrix

print("Modelo SVM entrenado y evaluado.")

# ... (código anterior de entrenamiento del modelo)

# 6. Predicciones sobre datos de prueba
y_pred = model.predict(X_test)

# Imprimir las predicciones
print("Predicciones:")
print(y_pred)

# Puedes comparar las predicciones con las etiquetas reales
print("Etiquetas reales:")
print(y_test.values)

import pandas as pd
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.svm import SVC
from sklearn.metrics import classification_report, confusion_matrix

# 1. Carga del dataset
df = pd.read_csv('transacciones_bancarias.csv')

# 2. Codificación de variables categóricas (One-Hot Encoding)
encoder = OneHotEncoder(handle_unknown='ignore', sparse_output=False)
encoded_features = encoder.fit_transform(df[['Ubicación', 'Tipo Cuenta', 'Método Pago']])
encoded_df = pd.DataFrame(encoded_features, columns=encoder.get_feature_names_out(['Ubicación', 'Tipo Cuenta', 'Método Pago']))
df = df.drop(['Ubicación', 'Tipo Cuenta', 'Método Pago'], axis=1)
df = pd.concat([df, encoded_df], axis=1)

# 3. División del dataset
X = df.drop('Fraude', axis=1)
y = df['Fraude']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

# 4. Escalado de datos
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# 5. Ajuste de hiperparámetros con GridSearchCV
param_grid = {'C': [0.1, 1, 10], 'gamma': [1, 0.1, 0.01], 'kernel': ['rbf', 'linear']}
grid = GridSearchCV(SVC(random_state=42), param_grid, refit=True, verbose=2)
grid.fit(X_train, y_train)
print("Mejores hiperparámetros:", grid.best_params_)

# 6. Evaluación del modelo con los mejores hiperparámetros
model = grid.best_estimator_
y_pred = model.predict(X_test)
print(confusion_matrix(y_test, y_pred))
print(classification_report(y_test, y_pred))

# 7. Interpretación de los resultados
# - Matriz de confusión: Analizar los verdaderos positivos, verdaderos negativos, falsos positivos y falsos negativos.
# - Precisión: Proporción de predicciones correctas entre las predicciones positivas.
# - Recall (Exhaustividad): Proporción de verdaderos positivos entre todos los casos positivos reales.
# - F1-score: Media armónica entre precisión y recall.
# - Otros: Considerar la curva ROC, el área bajo la curva ROC (AUC) para una evaluación más completa.

# Ejemplo de interpretación:
# - Si la precisión es alta pero el recall es bajo, el modelo puede estar clasificando correctamente las transacciones legítimas, pero fallando en detectar muchas transacciones fraudulentas.
# - Si el recall es alto pero la precisión es baja, el modelo puede estar detectando la mayoría de las transacciones fraudulentas, pero también clasificando erróneamente muchas transacciones legítimas como fraudulentas.
# - El F1-score proporciona un equilibrio entre precisión y recall.

# Ajustar la interpretación según los resultados específicos obtenidos.

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.svm import SVC
from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc
import matplotlib.pyplot as plt
from imblearn.over_sampling import SMOTE  # For oversampling

# 1. Carga del dataset
df = pd.read_csv('transacciones_bancarias.csv')

# 2. Ingeniería de características (Ejemplo: frecuencia de transacciones)
# (Necesitarías datos de tiempo para esto. Aquí se simula para demostrar la idea)
df['Frecuencia'] = np.random.randint(1, 10, df.shape[0])  # Simula frecuencia de 1 a 9

# 3. Codificación de variables categóricas
encoder = OneHotEncoder(handle_unknown='ignore', sparse_output=False)
encoded_features = encoder.fit_transform(df[['Ubicación', 'Tipo Cuenta', 'Método Pago']])
encoded_df = pd.DataFrame(encoded_features, columns=encoder.get_feature_names_out(['Ubicación', 'Tipo Cuenta', 'Método Pago']))
df = df.drop(['Ubicación', 'Tipo Cuenta', 'Método Pago'], axis=1)
df = pd.concat([df, encoded_df], axis=1)

# 4. Manejo de datos desbalanceados (Oversampling con SMOTE)
X = df.drop('Fraude', axis=1)
y = df['Fraude']
smote = SMOTE(random_state=42)
X_resampled, y_resampled = smote.fit_resample(X, y)

# 5. División del dataset (usando los datos resampleados)
X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42, stratify=y_resampled)

# 6. Escalado de datos
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# 7. Ajuste de hiperparámetros con GridSearchCV y validación cruzada
param_grid = {'C': [0.1, 1, 10], 'gamma': [1, 0.1, 0.01], 'kernel': ['rbf', 'linear']}
grid = GridSearchCV(SVC(random_state=42), param_grid, refit=True, verbose=2, cv=5, scoring='f1') # cv=5 para 5-fold CV, scoring='f1'
grid.fit(X_train, y_train)
print("Mejores hiperparámetros:", grid.best_params_)

# 8. Evaluación del modelo
model = grid.best_estimator_
y_pred = model.predict(X_test)
print(confusion_matrix(y_test, y_pred))
print(classification_report(y_test, y_pred))

# 9. Curva ROC y AUC
y_score = model.decision_function(X_test)  # Para SVM, usamos decision_function
fpr, tpr, _ = roc_curve(y_test, y_score)
roc_auc = auc(fpr, tpr)
plt.figure()
plt.plot(fpr, tpr, color='darkorange', lw=2, label='Curva ROC (área = %0.2f)' % roc_auc)
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('Tasa de Falsos Positivos')
plt.ylabel('Tasa de Verdaderos Positivos')
plt.title('Curva ROC')
plt.legend(loc="lower right")
plt.show()

# 10. Interpretación (ejemplo)
# (Ajustar la interpretación según los resultados)
#Hasta aqui me dejó Gemini Advance

from flask import Flask, request, jsonify
import pandas as pd
from sklearn.preprocessing import StandardScaler
import pickle  # For loading your model

app = Flask(__name__)

# Load your pre-trained model and scaler
with open('fraud_model.pkl', 'wb') as f:
    pickle.dump(model, f)
with open('scaler.pkl', 'wb') as f:
    pickle.dump(scaler, f)

print("Modelo y escalador guardados en archivos .pkl")

@app.route('/predict', methods=['POST'])
def predict():
    try:
        data = request.get_json()  # Get transaction data from request
        df = pd.DataFrame([data])  # Create a DataFrame

        # Feature Engineering (if any) - Add your code here
        # ...

        # Preprocessing (Scaling) - Transform the data using your scaler
        numerical_features = ['amount', 'transaction_count']  # Example numerical features
        df[numerical_features] = scaler.transform(df[numerical_features])

        # Prediction
        prediction = model.predict(df)[0] # Assuming only 1 prediction made
        probability = model.predict_proba(df)[0][1] # Probability of fraud

        return jsonify({'prediction': int(prediction), 'probability': float(probability)})
    except Exception as e:
        return jsonify({'error': str(e)}), 500  # Handle errors gracefully

if __name__ == '__main__':
    app.run(debug=True) # Set debug=False in production